<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-Joint-Conditional-and-Marginal-Probabilities">
    <title>Joint, Conditional and Marginal Probabilities</title>
    <p>
        In this section, we address probabilities in situation when you have more than one variable. Whether they are independent or not independent makes a difference. Let's first define a variable in the context of probability and statistics.
    </p>
    <subsection xml:id="subsec-Random-Variables">
        <title>Random Variables, Probabilities, and Expections</title>
        <p>
            We will think of variables as something the is observed or measured by experiments. The outcome in any experiment is a real value of the variable. A variable whose value is uncertain or unpredictable or varies from trial to trial, even though measurement conditions haven't changed, is called a random variable.
        </p>  
        <p>  
            We tend to use <alert>capital letter for the name</alert> of the variable and small letters for its values. Thus, for a variable <m>X</m>, the values will be denoted by <m>x_1, x_2, \cdots,</m> etc. Sometimes we will use superscripts to denote values, <m>x^{(1)},x^{(2)}, \cdots </m>. An <alert>event</alert> will now refer to the outcome that in a particular trial, variable <m>X</m> has some value or a set of the possible values. Thus, <m>X=\{x_1\}</m> would be an event and so would be <m>X=\{x_1, x_2\}</m>,  etc. In case of continuous values for <m>X</m>, and event may even be written as <m>x_1 \lt X \le x_2</m>, etc. We will speak about probabilities of these events in a later section.
        </p>
        <p>
            In our practice, the values of random variables will all be real numbers. Now, if <m>X</m> can take any real number, whether the entire real line or a finite segment of the real line, then we will call such a variable a <alert>continuous variable</alert>. An example of continuous variable will be price of a house, <m>X=\{x| 0 \le x \lt 1,000,000\}</m> if the price cannot be more than <m>1,000,000</m>. On the other hand, if the variable takes on only discrete values, then it will be called <alert>categorical or discrete variable</alert>. An example of discrete variable will fruits of interest in some grocery store <m>X=\{ \text{'apple'}, \text{'orange'}, \text{'pear'}, \text{'watermelon'}, \text{'grapes'} \}</m> which has five categories.
        </p>
        <p>
            The probability measure <m>P</m> of a probability space <m>(\Omega, F, P)</m> of a categorical variable has to specify probabilities of each of the elements of finite set <m>\Omega</m>. Let there be <m>N</m> distinct values of of a categorical variable <m>X</m>, say, <m>x_1, x_2, \cdots, x_N</m>. Then, all we need to specify <m>P</m> are the numbers <m>p_i</m> for each unique/exclusive outcome.
            <men xml:id="eqn-probability-mass-function">
                P(X=x_i) = p_i
            </men>
            Note that due to normalization, the probability of event <m>\Omega</m>, will be 1.
            <men xml:id="eqn-normalization-of-probability">
                P(\Omega) = \sum_{i=1}^N P(X=x_i) = \sum_{i=1}^N p_1 = 1.
            </men>
            
            This probability measure is called <alert>probability mass function</alert> or <m>PMF</m>. Given a PMF, it is easy to find the mean value of the random variable <m>X</m>, which will be denoted by angle brackets as per physics notation.
            <men xml:id="eqn-mean-of-random-variable">
                \text{Mean}(X) = \langle X \rangle = \sum_{i=1}^N  x_i\, P(X=x_i) = \sum_{i=1}^N\, x_i\, p_i.
            </men>
            The process of taking mean shows that it just sum of values of <m>X</m> weighted according to their probabilities. This weighting according to probabilities is called taking <alert>expectation</alert>. Thus, expectation value of any function <m>f(X)</m> is obtained accordingly.
            <men xml:id="eqn-expectation-values">
                \text{Expectation value of } f(X) \equiv \langle f(X) \rangle = \sum_{i=1}^N\, p_i\, f(x_i).
            </men>
            
            The variance will be expectation value of another variable obtained by subtracting the mean <m>\langle X \rangle</m> from the random variable <m>X</m> and squaring that. This gives us a measure of the spread of values about the mean.
            <men xml:id="eqn-variance-of-random-variable">
                \text{Var}(X) = \langle  \left( X - \langle X \rangle\right)^2 \rangle,
            </men>
            where <m>X</m> is the variable but <m>\langle X \rangle</m> is just a number. The following calculation will relate variance to <m>x_i</m> and <m>p_i</m>.
            <md>
                <mrow> \text{Var}(X) \amp = \sum_{i=1}^N \left( x_i - \langle X \rangle\right)^2\, P(X=x_i) </mrow>
                <mrow> \amp = \sum_{i=1}^N \left( x_i^2 - 2 x_i \langle X \rangle  + \langle X \rangle^2 \right)\, p_i </mrow>
                <mrow> \amp = \sum_{i=1}^N x_i^2 p_i - 2 \langle X \rangle\sum_{i=1}^N x_i p_i + \langle X \rangle^2\sum_{i=1}^N p_i  </mrow>
                <mrow>  \amp = \left( \sum_{i=1}^N p_i\,x_i^2 \right) - \langle X \rangle^2</mrow>
            </md>
            The first term is just expectation value of variable <m>X^2</m>, i.e., <m>\langle X^2 \rangle</m>. Therefore, we often see variance in the following simpler-looking formula.
            <men xml:id="eqn-variance-simpler">
                \text{Var}(X) = \langle X^2 \rangle - \left( \langle X \rangle \right)^2.          
            </men>
            The standard deviation will, of course be, just square root of variance.
            <men xml:id="eqn-standard-deviation-from-variance">
                \text{Stdev}(X) = \sqrt{ \text{Var}(X) }.
            </men>
        </p>
        
    </subsection>
    <subsection xml:id="subsec-Joint-Probability">
        <title>Joint ad Marginal Probabilities</title>
        <p>
            Suppose you have two discrete random variables, e.g, does the patient have a disease <m>(X)</m> and is the diagnosis positive <m>(Y)</m>. Both of these variables have just two values: <m>\Omega_X = \{ x_1 = \text{Yes}, x_2 = \text{No}\}</m> and <m>\Omega_Y = \{ y_1 = \text{Yes}, y_2 = \text{No}\}</m>
        </p>
        <p>
            <alert>Joint probability</alert> is probability measure over the outcome space <m>\Omega</m> that has all the combinations of the elements of <m>\Omega_X
            </m> and <m>\Omega_Y</m>. That is, we will specify probability measure for each of the elements of following <m>\Omega</m>
            <me>
                \Omega = \{ (x_1, y_1), (x_1, y_2), (x_2, y_1), (x_2, y_2) \}.
            </me>
            That is we need the following probabilities.
            <md>
                <mrow>p_{11} \amp = P(X=x_1, Y=y_1) </mrow>
                <mrow>p_{12} \amp = P(X=x_1, Y=y_2) </mrow>
                <mrow>p_{21} \amp = P(X=x_2, Y=y_1) </mrow>
                <mrow>p_{22} \amp = P(X=x_2, Y=y_2) </mrow>
            </md>
            These probabilities are joint probabilities over the joint space of <m>X</m> and <m>Y</m>. We often write joint probability by just listing the variable names or even a symbol for variable values, e.g., <m>x</m>, without specifying <m>x_1, x_2, \text{etc}</m>
            <me>
                \text{Joint Probability } \equiv P(X,Y) \equiv p(x,y).
            </me>
            
        </p>
        <p>
            If you ignore <m>Y</m> and just look at the probabilities of patient having the disease and not having the disease, they are called <alert>Marginal Probabilities</alert> of <m>X</m>. If you had more than two variables, say <m>Y, Z, A, B, \cdots</m>, you would just ignore all the other variables to focus only on <m>X</m> to obtain its marginal probabilities.
            <men xml:id="eqn-marginal-probability-of-X">
                P(x=x_1) = p_1,\ \ P(X=x_2) = p_2.
            </men>
            Similar to the notation of joint probabilities, we often write the marginal probabilities by simply <m>P(X)</m> or <m>p(x)</m>.
        </p>
        <p>
            It turns out that the marginal probabilities can be obtained from joint probabilities by just summing out the other variable's values in the joint probability. Thus, in our example, the p_1 and p_2 of <m>X</m> alone will be
            <md>
                <mrow>p_1 \amp = p_{11} + p_{12} \equiv \sum_{y=1}^2 P(X=1,Y=y) </mrow>
                <mrow>p_2 \amp = p_{21} + p_{22} \equiv \sum_{y=1}^2 P(X=2,Y=y) </mrow>
            </md>
            Same will be the case if you are interested in the marginal probability of <m>Y</m>.
        </p>
    </subsection>
    <subsection xml:id="subsec-Conditional-Probability">
        <title>Conditional Probability</title>
        <p>
            Conditional probability is a little tricky and is the most used in Machine Learning. Conditional probability is denoted by <m>P(X|Y)</m>. But, this is too abstract a notation. I will denote it by <m>P(X|Y=y)</m> to be a conditional probability  of <m>X</m> given that the random variable <m>Y</m> HAS A PARTICULAR VALUE <m>y</m>.
        </p>  
        <p>  
            It is very important to notice here that <m>P(X|Y=y)</m> IS A PROBABILITY OF X, just under some condition. That is, in the joint space <m>\Omega</m> of <m>X</m> and <m>Y</m>, you will collect all the points that correspond to a particular value of <m>Y</m>, say <m>Y=y_1</m>. Now, looking at only those, what can you say about the chances for different values of <m>X</m>. Clearly, conditional probability is trying to capture using some knowledge about the world that comes after you have known something about the world and thus, reducing the uncertainty. 
        </p>
        

    
    <!--<example>
        <statement> -->
            <p>
                <alert>Let us look at a numerical example: </alert>
                Suppose 1000 patients presented symptoms of a disease A. All patients were given a test. Only 200 of the tests came out positive. When later on, we find out which of the patients had the disease and whether test came out positive or not, we construct a table given below. Use it to figure out joint probabilities, marginal probabilities, and conditional probabilities.
            </p>
            <p>
                Before, we start writing formulas, it is best to pick a simple notation to reresent various events.
                <md>
                    <mrow>D \amp = \text{'Has disease'} </mrow>
                    <mrow>N \amp = \text{'Does not have disease'} </mrow>
                    <mrow>+ \amp = \text{'Test Positive'} </mrow>
                    <mrow>- \amp = \text{'Test Negative'} </mrow>
                </md>
            </p>
            <table xml:id="tab-patient-data-for-jt-and-conditional-probs">
                <title>Patient Data</title>
                <tabular>

                    <row>
                        <cell></cell><cell><m>+</m></cell><cell><m>-</m></cell> <cell></cell>
                    </row>
                    <row>
                        <cell><m>D</m></cell><cell><m>150</m></cell><cell><m>50</m></cell> <cell><m>n_D = 200</m></cell>
                    </row>
                    <row>
                        <cell><m>N</m></cell><cell><m>300</m></cell><cell><m>500</m></cell> <cell><m>n_N = 800</m></cell>
                    </row>    
                    <row>
                        <cell></cell><cell><m>n_+ = 450</m></cell><cell><m>n_{-}= 550</m></cell> <cell><m>n_T = 1000</m></cell>
                    </row>                
                </tabular>
            </table>
        <!-- </statement>
        <solution> -->
            <p>
                For Joint probabilities, we can just divide number in each square by the total number of patients, which <m>1000</m> here. By dividing the totals of rows <m>D</m> and <m>N</m>, we will get the marginals <m>P(D)</m> and <m>P(N)</m>. By dividing the column totals, we get the marginals <m>P(+)</m> and <m>P(-)</m>.
            <table xml:id="tab-jt-probs-disease-test">
                <title>Joint Probabilities</title>
                <tabular>

                    <row>
                        <cell></cell><cell><m>+</m></cell><cell><m>-</m></cell> <cell></cell>
                    </row>
                    <row>
                        <cell><m>D</m></cell><cell><m>0.150</m></cell><cell><m>0.050</m></cell> <cell><m>P(D) = 0.200</m></cell>
                    </row>
                    <row>
                        <cell><m>N</m></cell><cell><m>0.300</m></cell><cell><m>0.500</m></cell> <cell><m>P(N) = 800</m></cell>
                    </row>    
                    <row>
                        <cell></cell><cell><m>P(+) = 450</m></cell><cell><m>P(-) = 550</m></cell> <cell><m>P(\Omega) = 1.000</m></cell>
                    </row>                
                </tabular>
            </table>
            </p>

            <p>
                We use <alert>conditional probability</alert> answer questions like: what  is the probability of a patient having the disease if he has tested negative? When we look at <xref ref="tab-patient-data-for-jt-and-conditional-probs" />, look at only those cases where people test negative - that is, look at numbers in the 'Test Negative' column in the table. We work out other conditional probabilities similarly.
                <table>
                    <title>Patients that tested negative</title>
                    <tabular>
                        <row>
                            <cell>D</cell><cell>50</cell>
                        </row>
                        <row>
                            <cell>N</cell><cell>500</cell>
                        </row>
                    </tabular>
                </table>
                Now, we have a total of <m>50+500 = 550</m> cases of which <m>50</m> have disease. So, the <alert>conditional probability </alert> of a patient having the disease even when he has test negative will be
                <me>
                    p(D\ |\ -) = \frac{50}{550} = \frac{1}{11}.
                </me>
                That is <m>1</m> in <m>11</m> chance he has disease. Try to answer the question: what if a patient has tested positive, what is the probability that he doesn't have the disease, i.e., <m>P(N\ |\ +)</m>? Ans: <m>2/3</m>.
                
            </p>
            <p>
                You will have eight conditional probabilities in our simple example here:
                <md>
                    <mrow> \amp p(D\ |\ - ),\qquad p(N\ |\ - ) = 1 - p(D\ |\ - )</mrow>
                    <mrow> \amp p(D\ |\ + ),\qquad p(N\ |\ + ) = 1 - p(D\ |\ + )</mrow>
                    <mrow> \amp p(+\ |\ D ),\qquad p(-\ |\ D ) = 1 - p(+\ |\ D )</mrow>
                    <mrow> \amp p(+\ |\ N ),\qquad p(-\ |\ N ) = 1 - p(+\ |\ N )</mrow>
                </md>
                Try computing all of them! Here, in each row, you have the complementary event, hence you have to compute only four of them. It's important to recognize which event are complementary. For instance, <m>(D\ |\ -)</m> and <m>(N\ |\ -)</m> are complimentary, but <m>(+\ |\ D )</m> and <m>(+\ |\ N )</m> are not!
                <me>
                    p(+\ |\ D ) \ne 1 -  p(+\ |\ N ). 
                </me>
                That is because conditional probability <m>P(X|Y)</m> is a probability over the <m>X</m> space and not over the <m>Y</m> space, although the value of <m>Y=y_j</m> is important is choosing the slice of the joint probability. That is why, I like the notation <m>P(X|Y=y_j)</m> even though it is cumbersome to write.
                
            </p>
        
       <!-- </solution>
    </example> -->
    </subsection>


    <subsection xml:id="subsec-conditional-probability-from-joint-probability">
        <title>Bayes' Rule</title>
        <p>
            Before we discuss Bayes' rule, let's find the relation between Conditional Probability and Joint Probability. Recall that joint probability of two variables <m>X</m> and <m>Y</m> is probability over the space of <m>(X,Y)</m>-space, but the conditional probability of <m>X</m> given <m>Y=y_i</m>, <m>P(X,Y=y_i)</m> is over <m>X</m>-space on a lice of <m>P(X,Y)</m> with <m>Y=y_i</m>, where <m>y_i</m> is a particular value of <m>Y</m>. 
        </p>
        <p>
            By product rule of probabilities, it's clear that 
            <men xml:id="eqn-joint-prob-and-cond-prob-event">
                P(X=x_i, Y=y_j) = P(X=x_i|Y=y_j) P(Y=y_j).
            </men>
            You can write this as
            <me>
                 P(X=x_i|Y=y_j) = \frac{P(X=x_i, Y=y_j)}{ P(Y=y_j)}\ \ \left(  P(Y=y_j) \ne 0\right)
            </me>
            Often, we write this relation in a more general language rather then event by event.
            <men xml:id="eqn-joint-prob-and-cond-prob-general">
                P(X|Y) = \frac{P(X, Y)}{ P(Y)}.
            </men>
            The marginal <m>P(Y=y_j)</m> can, of course be obtained from the joint probability by marginalizing <m>X</m>, meaning summing over all <m>X</m>.
            <me>
                P(Y=y_j) = \sum_{x_i} P(X=x_i, Y=y_j).
            </me>
        </p>
        <p>
            Notice that on the left side of Eq. <xref ref="eqn-joint-prob-and-cond-prob-event"/>, the order of listing of <m>X</m> and <m>Y</m> values is arbitrary, i.e.,
            <me>
                 P(X=x_i, Y=y_j) =  P(Y=y_j, X=x_i).
            </me>
            Thus, it is equally possible to work with a given <m>X=x_i</m> and the associated conditional probability of <m>Y</m>, i.e., <m>P(Y=y_j | X=x_i)</m> and the marginal probability <m>P(X=x_i)</m>. This will give us
            <men xml:id="eqn-joint-prob-and-conditional-prob-second-way">
                 P(X=x_i, Y=y_j) = P(Y=y_j|X=x_i) P(X=x_i).
            </men>
            From Eqs. <xref ref="eqn-joint-prob-and-cond-prob-event"/> and <xref ref="eqn-joint-prob-and-conditional-prob-second-way"/>, we get the following relation.
            <men xml:id="eqn-Bayes-specific">
                 P(X=x_i|Y=y_j) P(Y=y_j) = P(Y=y_j|X=x_i) P(X=x_i).
            </men>
            Writing this in more general notation, we get the relation that is known as <alert>Bayes' theorem</alert> or <alert>Bayes' Rule</alert>.
            <men xml:id="eqn-Bayes-general">
                 P(X|Y) P(Y) = P(Y|X) P(X).
            </men>            
            It has become very important in the age of ML since many algorithms rely on it.
            
            
        </p>
        <p>
            Let us see <alert>an example of the application of Bayes' Rule</alert>. Suppose, in a population of
            <m>1\%</m> women in the age 30 to 40 develop breast cancer. It's also known that mammogram identifies <m>80\%</m> of cancers accurately and misses <m>20\%</m> of them. This means that if a woman has breast cancer, the test will be positive <m>80%</m> of the time. Furthermore, the test also give negative values <m>95\%</m> of the time correctly, i.e., if a woman doesn't have cancer, the test would came out negative <m>95%</m> of the times. 
        </p>  
        <p>  
            Now, a new woman in that age group comes in the lab and unfortunately, she is tested positive. So, what would you say about here chances of actually having the cancer? To answer this question, let us first introduce notations to simplify our formulas.
        </p>  
        <p>  
            Let 
            <md>
                <mrow> C \amp = \text{'Has cancer'} </mrow>
                <mrow> N \amp  = \text{'Does not have cancer'} </mrow>
                <mrow>  +  \amp = \text{'Test Positive'} </mrow>
                <mrow> -  \amp = \text{'Test Negative'} </mrow>
            </md>            
            
            
            Before, her mammogram, you would say that probability that she has cancer is just <m>1\%</m> since you don't know anything about her case other than her age group and you are just using the general knowledge. This is called  <alert>prior</alert> in the context of Bayes' rule. 
            <me>
                P(C) = 0.01.
            </me>
            This clearly says that the probability of the complement of Cancer is No Cancer. So,
            <me>
                P(N) = 1 - P(C) = 1 - 0.01 = 0.99.
            </me>            
            From the description, we also know the conditional probability that if a woman has breast cancer, her probability of test positive is <m>80%</m>.
            <me>
                P(+\ |\ C) = 0.80.
            </me>
            We have one more information in the data provided.
            <me>
                P(+\ |\ N) = 0.95.
            </me>            
            Now, we use Bayes' rule:
            <me>
                P(C\ |\ +) = \frac{ P(+\ |\ C) P(C) }{P(+)},
            </me>
            where 
            <me>
                P(+) = P(+\ |\ C) P(C) + P(+\ |\ N)P(N).
            </me>
            Let us use the numerical values now.
            <me>
                P(+) = 0.80 \times 0.01 + 0.95 \times 0.99 = 0.9485.
            </me>
            Therefore, our desired conditional probability will be
            <me>
                P(C\ |\ +) = \frac{ P(+\ |\ C) P(C) }{P(+)} = \frac{0.80 \times 0.01}{0.9485} = 0/084.
            </me>
            That is <m>8.4\%</m>.
        </p>  
            
        
    </subsection>


    <subsection xml:id="subsec-Independent-Variables">
        <title>Independent Variables</title>
        <p>
            Two random variables are said to be independent if their joint probability factors in the product of their marginal probabilities.
            <men xml:id="eqn-independent-variables">
                P(X,Y) = P(X) P(Y)\ \ \text{(Independent variables)}
            </men>
            Keep in mind that behind the scene, probabilities in this equation are over events, i.e., it's for <m>X</m> having some particular value and <m>Y</m> having its own particular value. If <m>X</m> and <m>Y</m> are independent variables, we aexpect
            <me>
                P(X = x_i\, \text{and}\, Y-+y_j) = P(X=x_i) P(Y=y_j)\ \ \text{for all }x_i \text{ and } y_j.
            </me>
            We wouldn't write our equations in the verbose manner, prefering to keep it simple. But, beware that probabilities are probabilities of events! 
            Now, we write the left side of Eq. <xref ref="eqn-independent-variables"/> using marginal and conditional probabilities.
            <me>
                P(X|Y)P(Y) = P(X) P(Y)\ \ \text{(Independent variables)}
            </me>
            Canceling <m>P(Y)</m> from both sides we get
            <me>
                 P(X|Y) = P(X)\ \ \text{(Independent variables)}
            </me>
            That is knowing something about <m>Y</m> does not tell you anything about probability of <m>X</m>. That is, conditioning on <m>Y</m> is useless when <m>X</m> and <m>Y</m> are independent.
            
            
        </p>
        
    </subsection>
    <subsection xml:id="subsec-Complications-For-Continuous-Variables">
        <title>Complications-For-Continuous-Variables</title>
        <p>
            In our discussions above, we spoke about probability space of a random variable <m>X</m> and spoke of probability of <m>X=x_i</m>, viz. <m>P(X=X_i)</m>. This works out nicely if <m>X</m> has only a finite number of discrete values, e.g., it is categorical variable. In that case we call <m>P(X</m> a <alert>probability mass function</alert>. However, when <m>X</m> is a continuous variable, say <m>0\le X\le 1</m>, then there are infinitely many values that <m>X</m> can take. In that case, probability for any one of the values will be zero. For instance, probability that <m>X=0.1</m> will be 0
            <me>
                P(X=0.1) = 0.
            </me>
            So, it is not a useful concept. We replace this probability with probability of <m>X</m> in a range, e.g.
            <me>
                P( 0.1 \le X \le 0.3 ), \text{Probability that outcome of the experiment will lie in the range of 0.1 to 0.3}.
            </me>
            To help with computing such probabilities, we introduce a concept of probability density <m>f(x)</m> such that probability of outcome of an experiment falling in an infinitesimal range <m>x \le X \le x+dx</m> will be
            <men xml:id="eqn-probability-density">
                P(x \le X \le x+dx) = f(x)\,dx.
            </men>
            Then, probability in a range will just be an integral over that range
            <me>
                P( 0.1 \le X \le 0.3 ) =  \int_{0.1}^{0.3}\, f(x)\,dx.
            </me>
            The probability density must be such that when integrated over all possible outcomes, i.e., the set <m>\Omega</m>, it should give a value of 1. Thus in the case of outcomes being in the range <m>0 \le X \le 1</m>, we require the following normalization condition.
            <men xml:id="eqn-probability-density-normalized">
                P( \Omega ) =  \int_{\text{full range}}\, f(x)\,dx = 1.
            </men>
            You may have heard of a Gaussian probability density of a variable <m>X</m>, whose range of possibilities is the entire real line, i.e., <m>-\infty \lt X \lt +\infty</m>, the probability density with mean <m>\mu</m> and standard deviation <m>\sigma</m> is
            <men xml:id="eqn-gaussian-probability-density">
                f(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\, e^{ -\frac{(x-\mu)^2}{2\sigma^2} },
            </men>
            where the factor in front of the exponential makes sure that ir is properly normalized to give 1.
            
        </p>
        
    </subsection>
</section>