<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="sec-Example-Probability-Distributions">
    <title>Example Probability Distributions</title>
    <introduction>
        <p>
            Distributions describe how probabilities are spread across values of a random variable. We will given examples of <alert>Probability Mass Function (PMF)</alert> for the discrete random variables and <alert>Probability Density Function (PDF)</alert> for continuous random variable.
        </p>
    </introduction>
    <subsection xml:id="subsec-Discrete-Random-Variables">
        <title>Discrete Random Variables</title>
        <subsubsection xml:id="subsubsec-Bernoulli-Distribution">
            <title>Bernoulli Distribution</title>
            <p>
                the outcome of each elementary event of a Bernoulli trial is either a failure or success of something 9false or true of some statement, or any myriads of two states problems, which is usually represented by a random variable <m>X</m> having values <m>0</m> and <m>1</m>. That is
                <men xml:id="eqn-Bernoulli-Omega">
                    \Omega = \{ X=0, X=1\}
                </men>
                We call such random variables <alert>Bernoulli variables</alert>. The values of probability of each value of <m>X</m> gives us the Probability Mass Function (PMF) of the Bernoulli distribution. Suppose probability of <m>(X=1)</m> is <m>p</m>. Then, probability of <m>(X=0)</m> will be <m>1-p</m>. 
                    <mdn>
                        <mrow xml:id="eqn-Bernoulli-X1-choice">P(X=1) \amp = p</mrow>
                        <mrow xml:id="eqn-Bernoulli-X0-choice">P(X=0) \amp = 1 - p \equiv q</mrow>
                    </mdn>
                Sometimes <m>1-p </m> is denote by <m>q</m>, with <m>p+q=1</m>. 
            </p> 
            
            <p>
                The separate listing of the probability of the two values of <m>X</m> in Eqs. <xref ref="eqn-Bernoulli-X1-choice"/> and <xref ref="eqn-Bernoulli-X0-choice"/> can actually be written more conveniently in one formula.
                <men xml:id="eqn-Bernoulli-prob-in-one-formula">
                    P(X=x) = p^x\:(1-p)^{1-x}.
                </men>
                From this formula, you will get <m>P(X=1)</m> and <m>P(X=0)</m> by substitting appropriate value of <m>x</m>.
                
            </p>
            <sidebyside widths="45% 10% 45%">

                <p>  
                    Graphically, Bernoulli distribution is plotted as bars with a dot at the top of the bar. Figure to the side shows an illustration with <m>p=0.6</m> for <m>X=1</m>, and of course, <m>q= 0.4</m> for <m>X=0</m>.
                </p>
                <p>
                      
                </p>
                <image source="./images/essential-probability-and-statistics/bernoulli.png">
                    <shortdescription>Bernoulli distribution with <m>p=0.6</m>.</shortdescription>
                </image>
            </sidebyside>

            <p>
                For any distribution, we can find the mean of variable <m>X</m> by weighing each value of <m>X</m> with its probability.
                <men xml:id="eqn-mean-of-Bernoulli-variable">
                    \langle X \rangle = 0 \times P(X=0) + 1\times P(X=1) = 0 + p = p
                </men>
                Similarly we can find the expectation value of any power of <m>X</m>. For instance, the expectation value of the <m>n^\text{th}</m> power of <m>X</m> will be
                <men xml:id="eqn-expectation-of-Bernoulli-variable">
                    \langle X^n \rangle = 0^n \times P(X=0) + 1^n\times P(X=1) = 0 + p = p
                </men>
                Thus, variance of a Bernoulli variable will be
                <men xml:id="eqn-Variance-of-Bernoulli-variable">
                    \text{Var}(X) = \langle X^2 \rangle - (\langle X \rangle)^2 = p - (p)^2 = p(1-p).
                </men>
                Therefore, the standard deviation, <m>\sigma</m> ,of Bernoulli variable is
                <men xml:id="eqn-stdev-Bernoulli-variable">
                    \sigma = \sqrt{ \text{Var}(X) } = \sqrt{p(1-p)}.
                </men>
            </p>

            
            
        </subsubsection>

        <subsubsection xml:id="subsubsec-Binomial-Distribution">
            <title>Binomial Distribution</title>
            <p>
                Imagine tossing a single coin a fixed number number of times, say <m>10</m> times. You might get no Heads at all or 1 Heads and 9 Tails, or 2 Heads and 8 Tails, etc. Record how many Heads you got in this trial, say you got 3 Heads. Now, toss the same coin 10 times again. This will be the second trial of experiment "tossing a particular coin 10 times". In the second trial, you might get a different number of Heads, say this time you got 8 Heads. 
            </p>
            <p>                  
                If you repeated the experiment above hundreds or thousands of times, you can build a table of number of trials that resulted in a total of <m>0</m> Heads, <m>1</m> Heads, <m>2</m> Heads, <m>\cdots</m>, <m>10</m> Heads, which are all the possibilities. This table, an example shown in <xref ref="tab-Binomial-frequency-and-prob"/> for 2000 trials, will be our <alert>Frequency Table</alert>. By dividing each of these numbers by the total number of trials you performed, you will get an estimate of probabilities of each outcome. The exact formula that gives the distribution you found is called Bernoulli distribution for the <alert><m>10</m>-toss experiment</alert>.
            </p>

            <table xml:id="tab-Binomial-frequency-and-prob">
                <title>Binomial Experiment: Frequency and Approximate Probability </title>
                <tabular>
                <row>
                    <cell>Total Number of Heads</cell>
                    <cell>Frequency</cell>
                    <cell>Approximate Probability</cell>
                </row>
                <row>
                    <cell>0</cell>
                    <cell>8</cell>
                    <cell>P(0) \approx 0.004</cell>
                </row>
                <row>
                    <cell>1</cell>
                    <cell>45</cell>
                    <cell>P(1) \approx 0.0225</cell>
                </row>
                <row>
                    <cell>2</cell>
                    <cell>120</cell>
                    <cell>P(2) \approx 0.06</cell>
                </row>
                <row>
                    <cell>3</cell>
                    <cell>220</cell>
                    <cell>P(3) \approx 0.11</cell>
                </row>
                <row>
                    <cell>4</cell>
                    <cell>300</cell>
                    <cell>P(4) \approx 0.15</cell>
                </row>
                <row>
                    <cell>5</cell>
                    <cell>350</cell>
                    <cell>P(5) \approx 0.175</cell>
                </row>
                <row>
                    <cell>6</cell>
                    <cell>300</cell>
                    <cell>P(6) \approx 0.15</cell>
                </row>
                <row>
                    <cell>7</cell>
                    <cell>250</cell>
                    <cell>P(7) \approx 0.125</cell>
                </row>
                <row>
                    <cell>8</cell>
                    <cell>200</cell>
                    <cell>P(8) \approx 0.10</cell>
                </row>
                <row>
                    <cell>9</cell>
                    <cell>150</cell>
                    <cell>P(9) \approx 0.075</cell>
                </row>
                <row>
                    <cell>10</cell>
                    <cell>57</cell>
                    <cell>P(10) \approx 0.0285</cell>
                </row>
                <row>
                    <cell>Total Number of Trials: </cell> <cell>2000</cell><cell></cell>
                </row>
                </tabular>
            </table>

            <p>
                In general, Binomial distribution gives us the probability of different number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success <m>p</m>. In each trial of a Binomial experiments you have a sequence of <m>N</m> Bernoulli repeats of <m>H</m> (for success) and <m>T</m> (for failure).
                <me>
                    \text{One Binomial Trial: } THTTTTHHTHHHTH...H\quad \text{(a total of N symbols.)}
                </me>
                Suppose this sequence has <m>m</m> Heads (<m>H</m>) and <m>N-m</m> Tails (<m>T</m>). The probability of this sequence of Bernoulli outcomes will be
                <me>
                    p^m\,(1-p)^{N-m}.
                </me>
                But the <m>H</m> and <m>T</m> could have occurred in any order.To get the probability of getting a total of <m>m</m> Heads in any order, we multiply number of different orders in which we could have got the same total number of Heads. That turns out to be the Binomial coefficient, and hence the name Binomial distribution. 
                <men xml:id="eqn-Binomial-distribution">
                    P(X = m; N, p) = \frac{N!}{m! (N-m)!}\,p^m\,(1-p)^{N-m}.
                </men>
                Note that this distribution has two <alert>fixed parameters</alert>, the number of independent Bernoulli trials <m>N</m> in each Binomial trial and <m>p</m>, the probability of <q>success</q> in each Bernoulli trial. 
                Beware of the importance of <m>N</m>; you can think of there being infinitely many Binomial distributions, each corresponding to different values of <m>N</m>. For instance, in Table <xref ref="tab-Binomial-frequency-and-prob"/>, if you had conducted the experiment with <m>15</m> Bernoulli trials in each Binomial experiment, instead of <m>10</m>, you would have gotten much different probabilities for <m>P(0),\ P(1), P(2), \cdots</m>. This is illustrated in the following figure, <xref ref="fig-binomial-10N20"/>.
            </p>
                <figure xml:id="fig-binomial-10N20">
                    <caption>Illustrating that different <m>N</m> values in Binomial distribution correspond to different distributions. Here, with <m>N=10</m> and <m>N=20</m>. See that the probabilities for same <m>X</m> value are different for the two distributions.</caption>
                    <image source="./images/essential-probability-and-statistics/binomial-10N20.png">
                        <shortdescription>llustrating that different <m>N</m> values in Binomial distribution correspond to different distributions. Here, with <m>N=10</m> and <m>N=20</m>. See that the probabilities for same <m>X</m> value are different for the two distributions.</shortdescription>
                    </image>
                </figure>
            <p>
                Following code was used to create the plot above.
            </p>
            <program language="python">
                <code>
                    import numpy as np
                    import matplotlib.pyplot as plt
                    from scipy.stats import binom

                    # Parameters
                    p = 0.5  # probability of success
                    N1 = 20  # number of trials for first distribution
                    N2 = 10  # number of trials for second distribution

                    # Support for each distribution
                    x1 = np.arange(0, N1+1)
                    x2 = np.arange(0, N2+1)

                    # PMFs
                    pmf1 = binom.pmf(x1, N1, p)
                    pmf2 = binom.pmf(x2, N2, p)

                    # Plot
                    fig, ax = plt.subplots(figsize=(8,5))

                    # Binomial N1
                    ax.vlines(x1, 0, pmf1, colors='blue', lw=2, label=f'Bin{N1}')
                    ax.plot(x1, pmf1, 'o', color='blue')

                    # Binomial N2
                    ax.vlines(x2, 0, pmf2, colors='orange', lw=2, label=f'Bin{N2}')
                    ax.plot(x2, pmf2, 'o', color='orange')

                    # Labels and grid
                    ax.set_title(f'Binomial Distribution PMFs (p={p})')
                    ax.set_xlabel('Number of Successes')
                    ax.set_ylabel('Probability')
                    ax.grid(axis='y', linestyle='--', alpha=0.6)
                    ax.legend()

                    plt.show()

                </code>
            </program>

            <p>
                Another way to improve your intuition about the Binomial distribution is to look at the impact of changing <m>p</m> value for the Bernoulli trials themselves - what impact do they have on a <m>10</m>-Binomial? It is shown in <xref ref="fig-binomial-N10p2p5pp8"/>. These plots show that low <m> p (0.2)</m> skews the PMF toward fewer successes; <m> p = 0.5</m> produces a symmetric distribution centered at <m>N/2</m>; high  <m>p (0.8)</m> skews toward more successes.

            </p>
            <figure xml:id="fig-binomial-N10p2p5pp8">
                <caption>Illustrating that different <m>p</m> values in Binomial distribution correspond to different distributions but  with <m>N=10</m>. </caption>
                <image source="./images/essential-probability-and-statistics/binomial-N10p2p5pp8.png">
                    <shortdescription>Illustrating that different <m>p</m> values in Binomial distribution correspond to different distributions but  with <m>N=10</m>.</shortdescription>
                </image>
            </figure>


            <p>
                For doing analytical calculations with the Binomial distribution, it is important to recall the following algebraic identity, called Binomial expansion.
                <men xml:id="eqn-Binomial-expansion">
                    (a + b)^N = \sum_{m=0}^{N}\, \frac{N!}{m! (N-m)!}\,a^m\,b^{N-m}.
                </men>
                Using this it is straightforward to show that Binomial distribution is normalized properly since
                <me>
                    \sum_{m=0}^{N}\,P(X = m; N, p) = \sum_{m=0}^{N}\,\frac{N!}{m! (N-m)!}\,p^m\,(1-p)^{N-m} = \left[ p + (1-p)\right]^N = 1.
                </me>
                The mean of the Binomial random variable <m>X</m> can be obtained by weighing each value of <m>X \in \{0, 1, 2, \cdots, N\} </m> by the corresponding probability.
                <men xml:id="eqn-men-Binomial-distribution">
                    \langle X \rangle = \sum_{m=0}^{N}\, m \, P(X = m; N, p) = N\,p.
                </men>
                A simple method of showing the result involves taking an appropriate derivative appropriately. 
                <md>
                    <mrow> \sum_{m=0}^{N}\,m\,\frac{N!}{m! (N-m)!}\,p^m\,(1-p)^{N-m} \amp  = \left[ p\frac{d}{dp}\sum_{m=0}^{N}\,\frac{N!}{m! (N-m)!}\,p^m\,q^{N-m} \right]_{q=1-p}</mrow>
                    <mrow> \amp = \left[ p\frac{d}{dp}(p + q)^N \right]_{q=1-p} </mrow>
                    <mrow> \amp = \left[p N(p + q)^{N-1} \right]_{q=1-p} = N\, p.</mrow>
                </md>
                
                The variance is similarly shown to be
                <men xml:id="eqn-variance-Binomial-distribution">
                    \text{Var}(X) = \langle X^2 \rangle - \langle X \rangle ^2 = N\,p\,(1-p).
                </men>
                And, the standard deviation <m>\sigma</m> is just the square root.
                <men xml:id="eqn-stdev-Binomial-distribution">
                    \sigma = \sqrt{\text{Var}(X) } = \sqrt{N\,p\,(1-p)}.
                </men>

            </p>
            <p>
                Binomial distribution plays important role in understanding average of several Bernoulli random variables, say <m>N</m>,  which have the same <m>p</m>. Suppose, we denote <m>N</m> Bernoulli variables by <m>X_1,\ X_2, \ \cdots\ , X_N</m>. Then, their sum will be a Binomial random variable, if Bernoulli random variables take <m>0</m> or <m>1</m> as awe have discussed above.
                <me>
                    X_\text{sum} = X_1 + X_2 + \cdots + X_N.
                </me>
                The average will be a scaled Binomial variable. We will denote this random variable by <m>\bar{X}_N</m> with a bar above the symbol and a reminder that it is average of <m>N</m> Bernoulli variables.
                <me>
                    \bar{X}_N = \frac{X_\text{sum}}{N} = \frac{X_1 + X_2 + \cdots + X_N}{N}.
                </me>
                This random variable is called <alert>sample mean</alert>. It will take the following values:
                <me>
                    \bar{X}_N \in \{  0, \frac{1}{N}, \frac{2}{N}, \cdots, \frac{N}{N}=1 \}
                </me>
                With <m>p</m> being probability of any of the individual Bernoulli variables to produce a success, i.e., <m>p = P(X_i = 1)</m> for every one of the <m>i = 1, 2, \cdots, N</m>.
                <me>
                    \langle \bar{X}_N \rangle = p,\qquad \text{independent of } N.
                </me>
                But the variance is quite interesting
                <me>
                    \text{Var}(\bar{X}_N) = \frac{p(1-p)}{N},
                </me>
                which translates to the standard deviation
                <me>
                    \sigma = \sqrt{ \text{Var}(\bar{X}_N) } = \frac{\sqrt{p(1-p)}}{\sqrt{N}}.
                </me>
                <alert>Where does this matter? </alert> 
            </p>
            <p> 
                <ul>
                    <li>
                        <p>
                            <alert>Estimation:</alert> In statistics, we often estimate <m>p</m> by <m>\langle \bar{X}_N \rangle</m> from data.

                        </p>
                    </li>
                    <li>
                        <p>
                            <alert>Interpretation: </alert> If you run multiple experiments, your average success rate will be centered at <m>p</m> and become more concentrated as <m>N</m> grows since the standard deviation drops as <m>\sim 1/\sqrt{N}</m>. This is illustrated in <xref ref="fig-bernoulli-to-CLT"/>.
                        </p>
                    </li>
                    <li>
                        <p>
                            <alert>Connection to the Central Limit Theorem:</alert> For large <m>N</m>, it can be shown that the probability distribution of the random variable <m>\bar{X}_N</m> tends to become Gaussian with the mean <m>p</m> and variance <m>p(1-p)/N</m>. We write this as
                            <me>
                                \bar{X}_N \approx \text{Gaussian}\left( p, \frac{p(1-p)}{N}\right),
                            </me>
                            even though each <m>X_i</m> is a discrete random variable. This goes by the name <alert>Central Limit Theorem</alert>.
                            

                        </p>
                    </li>
                </ul>
                <figure xml:id="fig-bernoulli-to-CLT">
                    <caption>Illustrating that for large <m>N</m> the distribution of the average of <m>N</m> Bernoulli variables of the same <m>p</m> tends towards a Gaussian distribution. </caption>
                    <image source="./images/essential-probability-and-statistics/bernoulli-to-CLT.png">
                        <shortdescription>Illustrating that for large <m>N</m> the distribution of the average of <m>N</m> Bernoulli variables of the same <m>p</m> tends towards a Gaussian distribution.</shortdescription>
                    </image>
                </figure> 

                    
                




                
                
                
                
                
                
                
                
            </p>
        </subsubsection>
        
    </subsection>

</section>